{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Enhanced RAG System with Dynamic Keyword Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Running the Interactive System\n",
    "\n",
    "\n",
    "\n",
    "First let's run the complete system. This will:\n",
    "\n",
    " 1. Initialize all components\n",
    "\n",
    " 2. Load and process the DT news from the `./data` directory\n",
    "\n",
    " 3. Create the vector store\n",
    "\n",
    " 4. Start the interactive Q&A interface\n",
    "\n",
    "\n",
    "\n",
    " **Important**: Make sure you have:\n",
    "\n",
    " - The directory with the DT news are in the same folder under the path `./data`\n",
    "\n",
    " - Installed all required dependencies\n",
    "\n",
    " - Ollama running with the Mistral model installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RAG system\n",
    "exec(open('enhanced_rag.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Documentation\n",
    "This section will provide insights into the workflow and the techniques used to build the entire system including RAG model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Table of Contents\n",
    "\n",
    " 1. [Introduction and Overview](#introduction)\n",
    "\n",
    " 2. [System Architecture and Dependencies](#architecture)\n",
    "\n",
    " 3. [Configuration and Setup](#configuration)\n",
    "\n",
    " 4. [Document Processing Pipeline](#document-processing)\n",
    "\n",
    " 5. [Vector Store Creation and Management](#vectorstore)\n",
    "\n",
    " 6. [Keyword Extraction and NLP Techniques](#keyword-extraction)\n",
    "\n",
    " 7. [Search Strategies and Retrieval](#search-strategies)\n",
    "\n",
    " 8. [Answer Generation and Refinement](#answer-generation)\n",
    "\n",
    " 9. [Interactive Question-Answering System](#interactive-qa)\n",
    "\n",
    "\n",
    "\n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='introduction'></a>\n",
    "\n",
    " ## 1. Introduction and Overview\n",
    "\n",
    "\n",
    " This notebook demonstrates an **Enhanced Retrieval-Augmented Generation (RAG) System** that combines multiple advanced techniques:\n",
    "\n",
    "\n",
    "\n",
    " - **Semantic Search**: Using sentence transformers for meaning-based document retrieval\n",
    "\n",
    " - **Dynamic Keyword Search**: Real-time keyword extraction and matching across documents\n",
    "\n",
    " - **Multi-Strategy Retrieval**: Combining vector similarity, keyword matching, and NLP-based search\n",
    "\n",
    " - **Intelligent Answer Refinement**: Two-stage answer generation with context enhancement\n",
    "\n",
    "\n",
    "\n",
    " The system is designed to provide accurate, context-aware answers by leveraging both traditional and modern NLP techniques.\n",
    "\n",
    "\n",
    " [Back to Contents](#table-of-contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='architecture'></a>\n",
    "\n",
    " ## 2. System Architecture and Dependencies\n",
    "\n",
    "\n",
    "\n",
    " ### Core Dependencies and Imports\n",
    "\n",
    "\n",
    "\n",
    " The system uses a modular architecture with fallback mechanisms for optional advanced features:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import time\n",
    "import textwrap\n",
    "\n",
    "# Core LangChain imports (try different import paths for compatibility)\n",
    "try:\n",
    "    from langchain.chains.retrieval import create_retrieval_chain\n",
    "    from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "    from langchain import hub\n",
    "    from langchain_community.llms import Ollama\n",
    "    from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "    from langchain_community.vectorstores import Chroma\n",
    "    from langchain_community.document_loaders import TextLoader\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    from langchain_core.documents import Document\n",
    "except ImportError:\n",
    "    # Fallback for older LangChain versions\n",
    "    try:\n",
    "        from langchain.chains.retrieval import create_retrieval_chain\n",
    "        from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "        from langchain import hub\n",
    "        from langchain.llms import Ollama\n",
    "        from langchain.embeddings import HuggingFaceEmbeddings\n",
    "        from langchain.vectorstores import Chroma\n",
    "        from langchain.document_loaders import TextLoader\n",
    "        from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "        from langchain_core.documents import Document\n",
    "    except ImportError:\n",
    "        print(\"Please install LangChain and its dependencies\")\n",
    "        raise\n",
    "\n",
    "# Advanced imports with fallbacks\n",
    "try:\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    ADVANCED_FEATURES = True\n",
    "except ImportError:\n",
    "    print(\"Advanced features require: pip install numpy scikit-learn sentence-transformers\")\n",
    "    ADVANCED_FEATURES = False\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    SPACY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SPACY_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from transformers import pipeline\n",
    "    TRANSFORMERS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TRANSFORMERS_AVAILABLE = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Architecture Overview\n",
    "\n",
    "\n",
    "\n",
    " The system architecture consists of:\n",
    "\n",
    "\n",
    "\n",
    " 1. **Document Processing Layer**: Handles various document formats and intelligent chunking\n",
    "\n",
    " 2. **Indexing Layer**: Creates both vector embeddings and keyword indices\n",
    "\n",
    " 3. **Retrieval Layer**: Multi-strategy search combining semantic and keyword-based approaches\n",
    "\n",
    " 4. **Generation Layer**: LLM-based answer generation with refinement capabilities\n",
    "\n",
    "\n",
    "\n",
    " [Back to Contents](#table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='configuration'></a>\n",
    "\n",
    " ## 3. Configuration and Setup\n",
    "\n",
    "\n",
    "\n",
    " ### Configuration Class\n",
    "\n",
    "\n",
    "\n",
    " The system uses a dataclass for centralized configuration management:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "@dataclass\n",
    "class RAGConfig:\n",
    "    \"\"\"Configuration class for RAG system parameters.\"\"\"\n",
    "    docs_dir: str = \"./data\"\n",
    "    vectorstore_path: str = \"./chroma_store\"\n",
    "    embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    llm_model: str = \"mistral\"\n",
    "    chunk_size: int = 1024\n",
    "    chunk_overlap: int = 200\n",
    "    initial_retrieval_k: int = 3\n",
    "    refinement_retrieval_k: int = 15\n",
    "    similarity_threshold: float = 0.7\n",
    "    max_retries: int = 3\n",
    "    log_level: str = \"INFO\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "[Back to Contents](#table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Main System Class Initialization\n",
    "\n",
    "\n",
    "\n",
    " The EnhancedRAGSystem class orchestrates all components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedRAGSystem:\n",
    "    \"\"\"\n",
    "    RAG system with document processing, \n",
    "    semantic search, and intelligent query refinement.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig):\n",
    "        self.config = config\n",
    "        self.setup_logging()\n",
    "        self.setup_models()\n",
    "        self.setup_llm_components()\n",
    "        self.document_cache = {}\n",
    "        self.text_files_content = {}  # Store content of all text files\n",
    "        self.document_index = {}  # Index for fast keyword search\n",
    "        \n",
    "    def setup_logging(self):\n",
    "        \"\"\"Setup logging system.\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=getattr(logging, self.config.log_level),\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def setup_models(self):\n",
    "        \"\"\"Initialize available models.\"\"\"\n",
    "        self.advanced_features = ADVANCED_FEATURES\n",
    "        self.spacy_available = SPACY_AVAILABLE\n",
    "        self.transformers_available = TRANSFORMERS_AVAILABLE\n",
    "        \n",
    "        if ADVANCED_FEATURES:\n",
    "            try:\n",
    "                self.sentence_model = SentenceTransformer(self.config.embedding_model)\n",
    "                self.logger.info(\"Sentence transformer model loaded successfully\")\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Failed to load sentence transformer: {e}\")\n",
    "                self.sentence_model = None\n",
    "        else:\n",
    "            self.sentence_model = None\n",
    "            \n",
    "        if SPACY_AVAILABLE:\n",
    "            try:\n",
    "                self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "                self.logger.info(\"spaCy model loaded successfully\")\n",
    "            except OSError:\n",
    "                self.logger.warning(\"spaCy model not found. Install with: python -m spacy download en_core_web_sm\")\n",
    "                self.nlp = None\n",
    "        else:\n",
    "            self.nlp = None\n",
    "            \n",
    "        if TRANSFORMERS_AVAILABLE:\n",
    "            try:\n",
    "                self.ner_pipeline = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "                self.logger.info(\"NER pipeline loaded successfully\")\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"NER pipeline initialization failed: {e}\")\n",
    "                self.ner_pipeline = None\n",
    "        else:\n",
    "            self.ner_pipeline = None\n",
    "            \n",
    "    def setup_llm_components(self):\n",
    "        \"\"\"Setup LLM and embedding components.\"\"\"\n",
    "        os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "        \n",
    "        self.embedding_model = HuggingFaceEmbeddings(\n",
    "            model_name=self.config.embedding_model,\n",
    "            model_kwargs={\"device\": \"cpu\"}\n",
    "        )\n",
    "        \n",
    "        self.llm = Ollama(\n",
    "            model=self.config.llm_model,\n",
    "            temperature=0.1\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [Back to Contents](#table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='document-processing'></a>\n",
    "\n",
    " ## 4. Document Processing Pipeline\n",
    "\n",
    "\n",
    "\n",
    " ### Document Loading and Indexing\n",
    "\n",
    "\n",
    "\n",
    " The system implements a sophisticated document processing pipeline that:\n",
    "\n",
    " 1. Loads documents from various formats\n",
    "\n",
    " 2. Creates keyword indices for fast search\n",
    "\n",
    " 3. Implements intelligent chunking strategies"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "    def load_and_process_documents(self) -> List[Document]:\n",
    "        \"\"\"Load and process documents with enhanced chunking and indexing.\"\"\"\n",
    "        self.logger.info(\"Loading and processing documents...\")\n",
    "        docs_path = Path(self.config.docs_dir)\n",
    "        \n",
    "        if not docs_path.exists():\n",
    "            raise FileNotFoundError(f\"Documents directory not found: {docs_path}\")\n",
    "            \n",
    "        all_docs = []\n",
    "        supported_formats = ['.txt'] # A folder with text files has been provided\n",
    "        \n",
    "        for file_path in docs_path.rglob('*'):\n",
    "            if file_path.suffix.lower() in supported_formats:\n",
    "                try:\n",
    "                    # Load raw content for keyword search\n",
    "                    raw_content = self._load_raw_content(file_path)\n",
    "                    if raw_content:\n",
    "                        self.text_files_content[str(file_path)] = raw_content\n",
    "                        self._build_document_index(str(file_path), raw_content)\n",
    "                    \n",
    "                    # Process for chunking\n",
    "                    docs = self._load_document(file_path)\n",
    "                    chunks = self._smart_chunking(docs, file_path)\n",
    "                    all_docs.extend(chunks)\n",
    "                    self.logger.info(f\"Processed {len(chunks)} chunks from {file_path}\")\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error processing {file_path}: {e}\")\n",
    "                    \n",
    "        self.logger.info(f\"Total documents processed: {len(all_docs)}\")\n",
    "        self.logger.info(f\"Text files indexed: {len(self.text_files_content)}\")\n",
    "        return all_docs\n",
    "    \n",
    "    def _load_raw_content(self, file_path: Path) -> str:\n",
    "        \"\"\"Load raw content from file for keyword indexing.\"\"\"\n",
    "        try:\n",
    "            if file_path.suffix.lower() == '.json':\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                if isinstance(data, dict):\n",
    "                    return ' '.join(str(value) for value in data.values() if isinstance(value, str))\n",
    "                elif isinstance(data, list):\n",
    "                    return ' '.join(str(item) for item in data if isinstance(item, str))\n",
    "                else:\n",
    "                    return str(data)\n",
    "            else:\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    return f.read()\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading raw content from {file_path}: {e}\")\n",
    "            return \"\"\n",
    "    \n",
    "    def _build_document_index(self, file_path: str, content: str):\n",
    "        \"\"\"Build an index for fast keyword search across all documents.\"\"\"\n",
    "        # Normalize content for better matching\n",
    "        normalized_content = content.lower()\n",
    "        \n",
    "        # Extract words (simple tokenization)\n",
    "        words = re.findall(r'\\b\\w+\\b', normalized_content)\n",
    "        \n",
    "        # Build word index\n",
    "        for word in words:\n",
    "            if len(word) > 2:  # Skip very short words\n",
    "                if word not in self.document_index:\n",
    "                    self.document_index[word] = []\n",
    "                if file_path not in self.document_index[word]:\n",
    "                    self.document_index[word].append(file_path)\n",
    "    \n",
    "    def _load_document(self, file_path: Path) -> List[Document]:\n",
    "        \"\"\"Load document based on file type.\"\"\"\n",
    "        if file_path.suffix.lower() == '.json':\n",
    "            return self._load_json_document(file_path)\n",
    "        else:\n",
    "            loader = TextLoader(str(file_path))\n",
    "            return loader.load()\n",
    "    \n",
    "    def _load_json_document(self, file_path: Path) -> List[Document]:\n",
    "        \"\"\"Load JSON document and convert to Document objects.\"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        documents = []\n",
    "        if isinstance(data, dict):\n",
    "            for key, value in data.items():\n",
    "                if isinstance(value, str):\n",
    "                    documents.append(Document(\n",
    "                        page_content=value,\n",
    "                        metadata={\"source\": str(file_path), \"key\": key}\n",
    "                    ))\n",
    "        elif isinstance(data, list):\n",
    "            for i, item in enumerate(data):\n",
    "                if isinstance(item, str):\n",
    "                    documents.append(Document(\n",
    "                        page_content=item,\n",
    "                        metadata={\"source\": str(file_path), \"index\": i}\n",
    "                    ))\n",
    "        return documents\n",
    "\n",
    "# Add methods to the EnhancedRAGSystem class\n",
    "EnhancedRAGSystem.load_and_process_documents = load_and_process_documents\n",
    "EnhancedRAGSystem._load_raw_content = _load_raw_content\n",
    "EnhancedRAGSystem._build_document_index = _build_document_index\n",
    "EnhancedRAGSystem._load_document = _load_document\n",
    "EnhancedRAGSystem._load_json_document = _load_json_document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Smart Chunking Strategies\n",
    "\n",
    "\n",
    "\n",
    " The system implements two chunking strategies:\n",
    "\n",
    " 1. **Semantic Chunking**: Uses NLP to maintain semantic boundaries\n",
    "\n",
    " 2. **Enhanced Traditional Chunking**: Fallback with improved separators"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "    def _smart_chunking(self, docs: List[Document], file_path: Path) -> List[Document]:\n",
    "        \"\"\"Enhanced chunking with multiple strategies.\"\"\"\n",
    "        if self.nlp and any(doc.page_content for doc in docs):\n",
    "            return self._semantic_chunking(docs, file_path)\n",
    "        else:\n",
    "            return self._enhanced_traditional_chunking(docs, file_path)\n",
    "    \n",
    "    def _semantic_chunking(self, docs: List[Document], file_path: Path) -> List[Document]:\n",
    "        \"\"\"Semantic-aware chunking using NLP.\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        for doc in docs:\n",
    "            try:\n",
    "                # Process with spaCy\n",
    "                nlp_doc = self.nlp(doc.page_content)\n",
    "                sentences = [sent.text for sent in nlp_doc.sents]\n",
    "                \n",
    "                current_chunk = \"\"\n",
    "                current_size = 0\n",
    "                \n",
    "                for sentence in sentences:\n",
    "                    sentence_size = len(sentence)\n",
    "                    \n",
    "                    if current_size + sentence_size > self.config.chunk_size and current_chunk:\n",
    "                        chunk_doc = Document(\n",
    "                            page_content=current_chunk.strip(),\n",
    "                            metadata={\n",
    "                                **doc.metadata,\n",
    "                                \"source\": str(file_path),\n",
    "                                \"chunk_type\": \"semantic\"\n",
    "                            }\n",
    "                        )\n",
    "                        chunks.append(chunk_doc)\n",
    "                        \n",
    "                        # Start new chunk with some overlap\n",
    "                        overlap_sentences = sentences[max(0, len(sentences) - 2):]\n",
    "                        current_chunk = \" \".join(overlap_sentences)\n",
    "                        current_size = len(current_chunk)\n",
    "                    else:\n",
    "                        current_chunk += \" \" + sentence\n",
    "                        current_size += sentence_size\n",
    "                \n",
    "                # Add final chunk\n",
    "                if current_chunk.strip():\n",
    "                    chunk_doc = Document(\n",
    "                        page_content=current_chunk.strip(),\n",
    "                        metadata={\n",
    "                            **doc.metadata,\n",
    "                            \"source\": str(file_path),\n",
    "                            \"chunk_type\": \"semantic\"\n",
    "                        }\n",
    "                    )\n",
    "                    chunks.append(chunk_doc)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Semantic chunking failed for {file_path}: {e}\")\n",
    "                # Fall back to traditional chunking\n",
    "                return self._enhanced_traditional_chunking(docs, file_path)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _enhanced_traditional_chunking(self, docs: List[Document], file_path: Path) -> List[Document]:\n",
    "        \"\"\"Enhanced traditional chunking with better separators\"\"\"\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.config.chunk_size,\n",
    "            chunk_overlap=self.config.chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \";\", \",\", \" \", \"\"]\n",
    "        )\n",
    "        \n",
    "        chunks = splitter.split_documents(docs)\n",
    "        \n",
    "        # Add enhanced metadata\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk.metadata.update({\n",
    "                \"source\": str(file_path),\n",
    "                \"chunk_type\": \"enhanced_traditional\",\n",
    "                \"chunk_index\": i\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "# Add methods to the EnhancedRAGSystem class\n",
    "EnhancedRAGSystem._smart_chunking = _smart_chunking\n",
    "EnhancedRAGSystem._semantic_chunking = _semantic_chunking\n",
    "EnhancedRAGSystem._enhanced_traditional_chunking = _enhanced_traditional_chunking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [Back to Contents](#table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='vectorstore'></a>\n",
    "\n",
    " ## 5. Vector Store Creation and Management\n",
    "\n",
    "\n",
    "\n",
    " ### Creating or Loading the Vector Store\n",
    "\n",
    "\n",
    "\n",
    " The system uses Chroma as the vector database with persistence capabilities:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "    def create_or_load_vectorstore(self, documents: List[Document]) -> Chroma:\n",
    "        \"\"\"Create or load vectorstore with error handling.\"\"\"\n",
    "        vectorstore_path = Path(self.config.vectorstore_path)\n",
    "        \n",
    "        try:\n",
    "            if vectorstore_path.exists():\n",
    "                self.logger.info(\"Loading existing vectorstore...\")\n",
    "                vectorstore = Chroma(\n",
    "                    persist_directory=str(vectorstore_path),\n",
    "                    embedding_function=self.embedding_model\n",
    "                )\n",
    "                # Test if vectorstore is working\n",
    "                vectorstore.similarity_search(\"test\", k=1)\n",
    "            else:\n",
    "                raise FileNotFoundError(\"Vectorstore not found\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.info(f\"Creating new vectorstore... (Error: {e})\")\n",
    "            vectorstore = Chroma.from_documents(\n",
    "                documents,\n",
    "                self.embedding_model,\n",
    "                persist_directory=str(vectorstore_path)\n",
    "            )\n",
    "            vectorstore.persist()\n",
    "            \n",
    "        return vectorstore\n",
    "\n",
    "# Add method to the EnhancedRAGSystem class\n",
    "EnhancedRAGSystem.create_or_load_vectorstore = create_or_load_vectorstore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [Back to Contents](#table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='keyword-extraction'></a>\n",
    "\n",
    " ## 6. Keyword Extraction and NLP Techniques\n",
    "\n",
    "\n",
    "\n",
    " ### Dynamic Keyword Search Implementation\n",
    "\n",
    "\n",
    "\n",
    " The system implements a sophisticated keyword search mechanism:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "    def search_keywords_in_documents(self, keywords: List[str]) -> List[Document]:\n",
    "        \"\"\"Search for keywords across all loaded text files.\"\"\"\n",
    "        matching_docs = []\n",
    "        \n",
    "        for keyword in keywords:\n",
    "            if len(keyword) < 2:\n",
    "                continue\n",
    "                \n",
    "            keyword_lower = keyword.lower()\n",
    "            \n",
    "            # Search in document index for exact matches\n",
    "            if keyword_lower in self.document_index:\n",
    "                for file_path in self.document_index[keyword_lower]:\n",
    "                    content = self.text_files_content.get(file_path, \"\")\n",
    "                    if content and keyword_lower in content.lower():\n",
    "                        # Extract relevant context around the keyword\n",
    "                        context = self._extract_context_around_keyword(content, keyword, context_size=500)\n",
    "                        matching_docs.append(Document(\n",
    "                            page_content=context,\n",
    "                            metadata={\n",
    "                                \"source\": file_path,\n",
    "                                \"keyword\": keyword,\n",
    "                                \"search_type\": \"keyword_index\"\n",
    "                            }\n",
    "                        ))\n",
    "            \n",
    "            # Also search for partial matches in all documents\n",
    "            for file_path, content in self.text_files_content.items():\n",
    "                if keyword_lower in content.lower():\n",
    "                    # Skip if already found in index search\n",
    "                    if keyword_lower in self.document_index and file_path in self.document_index[keyword_lower]:\n",
    "                        continue\n",
    "                    \n",
    "                    context = self._extract_context_around_keyword(content, keyword, context_size=500)\n",
    "                    matching_docs.append(Document(\n",
    "                        page_content=context,\n",
    "                        metadata={\n",
    "                            \"source\": file_path,\n",
    "                            \"keyword\": keyword,\n",
    "                            \"search_type\": \"partial_match\"\n",
    "                        }\n",
    "                    ))\n",
    "        \n",
    "        return matching_docs\n",
    "    \n",
    "    def _extract_context_around_keyword(self, content: str, keyword: str, context_size: int = 500) -> str:\n",
    "        \"\"\"Extract context around a keyword from the content.\"\"\"\n",
    "        keyword_lower = keyword.lower()\n",
    "        content_lower = content.lower()\n",
    "        \n",
    "        # Find the position of the keyword\n",
    "        pos = content_lower.find(keyword_lower)\n",
    "        if pos == -1:\n",
    "            return content[:context_size]  # Return beginning if not found\n",
    "        \n",
    "        # Calculate start and end positions for context\n",
    "        start = max(0, pos - context_size // 2)\n",
    "        end = min(len(content), pos + len(keyword) + context_size // 2)\n",
    "        \n",
    "        # Try to start and end at word boundaries\n",
    "        while start > 0 and content[start] not in ' \\n\\t':\n",
    "            start -= 1\n",
    "        while end < len(content) and content[end] not in ' \\n\\t':\n",
    "            end += 1\n",
    "        \n",
    "        context = content[start:end].strip()\n",
    "        \n",
    "        # Add ellipsis if we're not at the beginning/end\n",
    "        if start > 0:\n",
    "            context = \"...\" + context\n",
    "        if end < len(content):\n",
    "            context = context + \"...\"\n",
    "        \n",
    "        return context\n",
    "\n",
    "# Add methods to the EnhancedRAGSystem class\n",
    "EnhancedRAGSystem.search_keywords_in_documents = search_keywords_in_documents\n",
    "EnhancedRAGSystem._extract_context_around_keyword = _extract_context_around_keyword\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Multi-Strategy Keyword Extraction\n",
    "\n",
    "\n",
    "\n",
    " The system combines multiple techniques for keyword extraction:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "    def extract_keywords_with_llm(self, text: str) -> Dict[str, List[str]]:\n",
    "        \"\"\"Extract keywords using LLM with multiple strategies.\"\"\"\n",
    "        try:\n",
    "            # Try structured extraction first\n",
    "            if self.ner_pipeline:\n",
    "                entities = self.ner_pipeline(text)\n",
    "                entity_words = [entity[\"word\"] for entity in entities if len(entity[\"word\"]) > 2]\n",
    "            else:\n",
    "                entity_words = []\n",
    "            \n",
    "            # Use spaCy if available\n",
    "            if self.nlp:\n",
    "                doc = self.nlp(text)\n",
    "                spacy_entities = [ent.text for ent in doc.ents]\n",
    "                keywords = [\n",
    "                    token.lemma_.lower() for token in doc \n",
    "                    if not token.is_stop and not token.is_punct and len(token.text) > 2\n",
    "                ]\n",
    "            else:\n",
    "                spacy_entities = []\n",
    "                keywords = []\n",
    "            \n",
    "            # LLM-based extraction as fallback or enhancement\n",
    "            llm_extraction = self._llm_keyword_extraction(text)\n",
    "            \n",
    "            # Combine all results\n",
    "            all_entities = list(set(entity_words + spacy_entities + llm_extraction.get(\"entities\", [])))\n",
    "            all_keywords = list(set(keywords + llm_extraction.get(\"keywords\", [])))\n",
    "            \n",
    "            return {\n",
    "                \"entities\": all_entities,\n",
    "                \"keywords\": all_keywords,\n",
    "                \"concepts\": llm_extraction.get(\"concepts\", [])\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Keyword extraction failed: {e}\")\n",
    "            return {\"entities\": [], \"keywords\": [], \"concepts\": []}\n",
    "    \n",
    "    def _llm_keyword_extraction(self, text: str) -> Dict[str, List[str]]:\n",
    "        \"\"\"LLM-based keyword extraction with structured output.\"\"\"\n",
    "        try:\n",
    "            prompt = f\"\"\"Extract key information from this text and return it in JSON format:\n",
    "            \n",
    "Text: {text}\n",
    "\n",
    "Please extract:\n",
    "1. entities: specific names of people, organizations, locations, products\n",
    "2. keywords: important domain-specific terms and concepts\n",
    "3. concepts: abstract ideas and themes\n",
    "\n",
    "Return only valid JSON in this format:\n",
    "{{\"entities\": [\"example1\", \"example2\"], \"keywords\": [\"keyword1\", \"keyword2\"], \"concepts\": [\"concept1\", \"concept2\"]}}\n",
    "\n",
    "Example:\n",
    "{{\"entities\": [\"Deutsche Telekom\", \"AI\", \"CEO\"], \"keywords\": [\"investment\", \"artificial intelligence\", \"technology\"], \"concepts\": [\"digital transformation\", \"innovation\"]}}\n",
    "\"\"\"\n",
    "            \n",
    "            response = self.llm.invoke(prompt)\n",
    "            \n",
    "            # Try to parse JSON from response\n",
    "            try:\n",
    "                # Look for JSON in the response\n",
    "                json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "                if json_match:\n",
    "                    json_str = json_match.group(0)\n",
    "                    result = json.loads(json_str)\n",
    "                    return result\n",
    "                else:\n",
    "                    return {\"entities\": [], \"keywords\": [], \"concepts\": []}\n",
    "                    \n",
    "            except json.JSONDecodeError:\n",
    "                # If JSON parsing fails, try simple extraction\n",
    "                return self._simple_keyword_extraction(text)\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"LLM keyword extraction failed: {e}\")\n",
    "            return {\"entities\": [], \"keywords\": [], \"concepts\": []}\n",
    "    \n",
    "    def _simple_keyword_extraction(self, text: str) -> Dict[str, List[str]]:\n",
    "        \"\"\"Simple keyword extraction fallback.\"\"\"\n",
    "        words = re.findall(r'\\b[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\b', text)\n",
    "        keywords = []\n",
    "        \n",
    "        # Extract potential keywords based on capitalization and length\n",
    "        for word in words:\n",
    "            if len(word) > 2 and word.lower() not in ['the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by']:\n",
    "                keywords.append(word.lower())\n",
    "        \n",
    "        return {\n",
    "            \"entities\": list(set(words)),\n",
    "            \"keywords\": list(set(keywords)),\n",
    "            \"concepts\": []\n",
    "        }\n",
    "\n",
    "# Add methods to the EnhancedRAGSystem class\n",
    "EnhancedRAGSystem.extract_keywords_with_llm = extract_keywords_with_llm\n",
    "EnhancedRAGSystem._llm_keyword_extraction = _llm_keyword_extraction\n",
    "EnhancedRAGSystem._simple_keyword_extraction = _simple_keyword_extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [Back to Contents](#table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='search-strategies'></a>\n",
    "\n",
    " ## 7. Search Strategies and Retrieval\n",
    "\n",
    "\n",
    "\n",
    " ### Semantic Search Implementation\n",
    "\n",
    "\n",
    "\n",
    " The system implements semantic search using sentence transformers with fallback options:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "    def semantic_search(self, query: str, documents: List[Document], top_k: int = 10) -> List[Document]:\n",
    "        \"\"\"Semantic search with fallback to simple similarity.\"\"\"\n",
    "        if not documents:\n",
    "            return []\n",
    "        \n",
    "        if self.sentence_model:\n",
    "            try:\n",
    "                # Use sentence transformer for semantic search\n",
    "                query_embedding = self.sentence_model.encode([query])\n",
    "                doc_texts = [doc.page_content for doc in documents]\n",
    "                doc_embeddings = self.sentence_model.encode(doc_texts)\n",
    "                \n",
    "                similarities = cosine_similarity(query_embedding, doc_embeddings)[0]\n",
    "                \n",
    "                # Create scored documents\n",
    "                scored_docs = []\n",
    "                for i, doc in enumerate(documents):\n",
    "                    score = similarities[i]\n",
    "                    if score >= self.config.similarity_threshold:\n",
    "                        scored_docs.append((doc, score))\n",
    "                \n",
    "                # Sort by score and return top_k\n",
    "                scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "                return [doc for doc, _ in scored_docs[:top_k]]\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Semantic search failed: {e}\")\n",
    "        \n",
    "        # Fallback to simple text matching\n",
    "        return self._simple_text_search(query, documents, top_k)\n",
    "    \n",
    "    def _simple_text_search(self, query: str, documents: List[Document], top_k: int) -> List[Document]:\n",
    "        \"\"\"Simple text-based search as fallback\"\"\"\n",
    "        query_words = set(query.lower().split())\n",
    "        scored_docs = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            doc_words = set(doc.page_content.lower().split())\n",
    "            overlap = len(query_words.intersection(doc_words))\n",
    "            if overlap > 0:\n",
    "                score = overlap / len(query_words)\n",
    "                scored_docs.append((doc, score))\n",
    "        \n",
    "        scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [doc for doc, _ in scored_docs[:top_k]]\n",
    "\n",
    "# Add methods to the EnhancedRAGSystem class\n",
    "EnhancedRAGSystem.semantic_search = semantic_search\n",
    "EnhancedRAGSystem._simple_text_search = _simple_text_search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [Back to Contents](#table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='answer-generation'></a>\n",
    "\n",
    " ## 8. Answer Generation and Refinement\n",
    "\n",
    "\n",
    "\n",
    " ### Two-Stage Answer Generation Process\n",
    "\n",
    "\n",
    "\n",
    " The system uses a sophisticated two-stage process for answer generation:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "    def answer_question_with_refinement(self, question: str) -> Dict[str, Union[str, List[str]]]:\n",
    "        \"\"\"Main method to answer questions with refinement using dynamic keyword search\"\"\"\n",
    "        start_time = time.time()\n",
    "        self.logger.info(f\"Processing question: {question}\")\n",
    "        \n",
    "        try:\n",
    "            # Stage 1: Initial retrieval and answer\n",
    "            initial_answer, initial_sources = self._get_initial_answer(question)\n",
    "            \n",
    "            # Stage 2: Extract keywords and entities\n",
    "            extracted_info = self.extract_keywords_with_llm(question)\n",
    "            \n",
    "            # Stage 3: Enhanced document retrieval using dynamic keyword search\n",
    "            enhanced_docs = self._enhanced_document_retrieval(question, extracted_info)\n",
    "            \n",
    "            # Stage 4: Refine answer if additional relevant documents identified\n",
    "            if enhanced_docs:\n",
    "                final_answer = self._refine_answer(question, initial_answer, enhanced_docs)\n",
    "            else:\n",
    "                final_answer = initial_answer\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            self.logger.info(f\"Question processed in {processing_time:.2f} seconds\")\n",
    "            \n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"answer\": final_answer,\n",
    "                \"sources\": list(set([doc.metadata.get(\"source\", \"Unknown\") for doc in enhanced_docs + initial_sources])),\n",
    "                \"entities\": extracted_info[\"entities\"],\n",
    "                \"keywords\": extracted_info[\"keywords\"],\n",
    "                \"processing_time\": processing_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing question: {e}\")\n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"answer\": \"System encountered an error while processing the question. Please try again.\",\n",
    "                \"sources\": [],\n",
    "                \"entities\": [],\n",
    "                \"keywords\": [],\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def _get_initial_answer(self, question: str) -> Tuple[str, List[Document]]:\n",
    "        \"\"\"Get initial answer using standard RAG pipeline\"\"\"\n",
    "        try:\n",
    "            retriever = self.vectorstore.as_retriever(\n",
    "                search_kwargs={\"k\": self.config.initial_retrieval_k}\n",
    "            )\n",
    "            \n",
    "            prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "            document_chain = create_stuff_documents_chain(self.llm, prompt)\n",
    "            qa_chain = create_retrieval_chain(retriever, document_chain)\n",
    "            \n",
    "            result = qa_chain.invoke({\"input\": question})\n",
    "            \n",
    "            return result['answer'], result.get('context', [])\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Initial answer retrieval failed: {e}\")\n",
    "            return \"The request has not been processed\", []\n",
    "    \n",
    "    def _enhanced_document_retrieval(self, question: str, extracted_info: Dict) -> List[Document]:\n",
    "        \"\"\"Enhanced document retrieval using multiple strategies including dynamic keyword search\"\"\"\n",
    "        enhanced_docs = []\n",
    "        \n",
    "        # Strategy 1: Semantic search if loaded above\n",
    "        if hasattr(self, 'all_documents') and self.all_documents:\n",
    "            semantic_results = self.semantic_search(question, self.all_documents, top_k=5)\n",
    "            enhanced_docs.extend(semantic_results)\n",
    "        \n",
    "        # Strategy 2: Dynamic keyword search across all loaded text files\n",
    "        all_search_terms = extracted_info[\"entities\"] + extracted_info[\"keywords\"] + extracted_info[\"concepts\"]\n",
    "        if all_search_terms:\n",
    "            keyword_results = self.search_keywords_in_documents(all_search_terms)\n",
    "            enhanced_docs.extend(keyword_results)\n",
    "            self.logger.info(f\"Found {len(keyword_results)} documents through keyword search\")\n",
    "        \n",
    "        # Strategy 3: Vector similarity search for each entity/keyword\n",
    "        for term in all_search_terms:\n",
    "            if len(term) > 2:\n",
    "                try:\n",
    "                    results = self.vectorstore.similarity_search(term, k=2)\n",
    "                    enhanced_docs.extend(results)\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"Vector search failed for '{term}': {e}\")\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        unique_docs = []\n",
    "        seen_content = set()\n",
    "        \n",
    "        for doc in enhanced_docs:\n",
    "            # Create a hash based on content and source\n",
    "            content_hash = hash((doc.page_content, doc.metadata.get(\"source\", \"\")))\n",
    "            if content_hash not in seen_content:\n",
    "                seen_content.add(content_hash)\n",
    "                unique_docs.append(doc)\n",
    "        \n",
    "        # Sort by relevance (keyword matches get higher priority)\n",
    "        unique_docs.sort(key=lambda x: (\n",
    "            x.metadata.get(\"search_type\", \"other\") == \"keyword_index\",\n",
    "            x.metadata.get(\"search_type\", \"other\") == \"partial_match\"\n",
    "        ), reverse=True)\n",
    "        \n",
    "        return unique_docs[:self.config.refinement_retrieval_k]\n",
    "    \n",
    "    def _refine_answer(self, question: str, initial_answer: str, enhanced_docs: List[Document]) -> str:\n",
    "        \"\"\"Refine the answer using enhanced document context\"\"\"\n",
    "        if not enhanced_docs:\n",
    "            return initial_answer\n",
    "        \n",
    "        # Create context from enhanced documents\n",
    "        context_parts = []\n",
    "        for doc in enhanced_docs:\n",
    "            if len(\"\\n\".join(context_parts)) < 3000:  # Prevent context overflow\n",
    "                source = doc.metadata.get('source', 'Unknown')\n",
    "                keyword = doc.metadata.get('keyword', '')\n",
    "                search_type = doc.metadata.get('search_type', '')\n",
    "                \n",
    "                context_parts.append(f\"Source: {source}\")\n",
    "                if keyword:\n",
    "                    context_parts.append(f\"Keyword match: {keyword}\")\n",
    "                if search_type:\n",
    "                    context_parts.append(f\"Search type: {search_type}\")\n",
    "                context_parts.append(doc.page_content)\n",
    "                context_parts.append(\"---\")\n",
    "        \n",
    "        context = \"\\n\".join(context_parts)\n",
    "        \n",
    "        # Create refinement prompt\n",
    "        refinement_prompt = f\"\"\"You are an expert assistant. Please provide a comprehensive answer to the question based on the original answer and additional context from multiple text files.\n",
    "\n",
    "Original Question: {question}\n",
    "\n",
    "Initial Answer: {initial_answer}\n",
    "\n",
    "Additional Context from Text Files:\n",
    "{context}\n",
    "\n",
    "Please provide an improved answer that:\n",
    "1. Incorporates relevant information from the additional context\n",
    "2. Maintains accuracy and coherence\n",
    "3. Provides specific details and examples where available\n",
    "4. Answers the question directly and thoroughly\n",
    "5. Synthesizes information from multiple sources when relevant\n",
    "\n",
    "If the additional context doesn't improve the answer, you may return the original answer.\n",
    "\n",
    "Improved Answer:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            refined_answer = self.llm.invoke(refinement_prompt)\n",
    "            \n",
    "            # Basic validation\n",
    "            if refined_answer and len(refined_answer.strip()) > 20:\n",
    "                return refined_answer\n",
    "            else:\n",
    "                return initial_answer\n",
    "                \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Answer refinement failed: {e}\")\n",
    "            return initial_answer # Will return first answer\n",
    "\n",
    "# Add methods to the EnhancedRAGSystem class\n",
    "EnhancedRAGSystem.answer_question_with_refinement = answer_question_with_refinement\n",
    "EnhancedRAGSystem._get_initial_answer = _get_initial_answer\n",
    "EnhancedRAGSystem._enhanced_document_retrieval = _enhanced_document_retrieval\n",
    "EnhancedRAGSystem._refine_answer = _refine_answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [Back to Contents](#table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <a id='interactive-qa'></a>\n",
    "\n",
    " ## 9. Interactive Question-Answering System\n",
    "\n",
    "\n",
    "\n",
    " ### System Initialization\n",
    "\n",
    "\n",
    "\n",
    " Initialize the complete RAG system:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "    def initialize_system(self):\n",
    "        \"\"\"Initialize the complete RAG system\"\"\"\n",
    "        self.logger.info(\"Initializing RAG System...\")\n",
    "        \n",
    "        # Load and process documents\n",
    "        self.all_documents = self.load_and_process_documents()\n",
    "        \n",
    "        # Create vectorstore\n",
    "        self.vectorstore = self.create_or_load_vectorstore(self.all_documents)\n",
    "        \n",
    "        self.logger.info(\"System initialization complete\")\n",
    "        \n",
    "        # Log system capabilities\n",
    "        capabilities = []\n",
    "        if self.sentence_model:\n",
    "            capabilities.append(\"Semantic Search\")\n",
    "        if self.nlp:\n",
    "            capabilities.append(\"NLP Processing\")\n",
    "        if self.ner_pipeline:\n",
    "            capabilities.append(\"Named Entity Recognition\")\n",
    "        if self.text_files_content:\n",
    "            capabilities.append(\"Dynamic Keyword Search\")\n",
    "        \n",
    "        self.logger.info(f\"Available capabilities: {', '.join(capabilities) if capabilities else 'Basic RAG'}\")\n",
    "        self.logger.info(f\"Indexed {len(self.text_files_content)} text files for keyword search\")\n",
    "        self.logger.info(f\"Document index contains {len(self.document_index)} unique terms\")\n",
    "\n",
    "# Add method to the EnhancedRAGSystem class\n",
    "EnhancedRAGSystem.initialize_system = initialize_system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Main Execution Function\n",
    "\n",
    "\n",
    "\n",
    " The main function that runs the interactive Q&A system:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"Enhanced RAG System with Dynamic Keyword Search\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Configuration\n",
    "    config = RAGConfig(\n",
    "        docs_dir=\"./data\",  # Update this path as needed\n",
    "        llm_model=\"mistral\",\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=50,\n",
    "        initial_retrieval_k=3,\n",
    "        refinement_retrieval_k=10,\n",
    "        similarity_threshold=0.7,\n",
    "        log_level=\"INFO\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Initialize system\n",
    "        rag_system = EnhancedRAGSystem(config)\n",
    "        rag_system.initialize_system()\n",
    "        \n",
    "        # Interactive mode\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"SYSTEM READY - Enter your questions (type 'quit', 'exit', 'q' to exit)\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        while True:\n",
    "            question = input(\"\\nQuestion: \").strip()\n",
    "            \n",
    "            if question.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"Session ended\")\n",
    "                break\n",
    "                \n",
    "            if not question:\n",
    "                continue\n",
    "                \n",
    "            print(\"\\nProcessing...\")\n",
    "            result = rag_system.answer_question_with_refinement(question)\n",
    "            \n",
    "            # Display results\n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"RESULTS\")\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"Question: {result['question']}\")\n",
    "            answer_wrap = textwrap.fill(result['answer'], width=100)\n",
    "            print(f\"\\nAnswer: {answer_wrap}\")\n",
    "            \n",
    "            if result['sources']:\n",
    "                print(f\"\\nSources:\")\n",
    "                for source in result['sources']:\n",
    "                    print(f\"  - {source}\")\n",
    "            \n",
    "            if result['entities']:\n",
    "                print(f\"\\nEntities found: {', '.join(result['entities'])}\")\n",
    "            \n",
    "            if result['keywords']:\n",
    "                print(f\"Keywords found: {', '.join(result['keywords'])}\")\n",
    "            \n",
    "            print(f\"\\nProcessing Time: {result.get('processing_time', 0):.2f} seconds\")\n",
    "            \n",
    "            if 'error' in result:\n",
    "                print(f\"Error: {result['error']}\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nSystem interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"System initialization failed: {e}\")\n",
    "        print(\"Please check your configuration and dependencies.\")\n",
    "        print(\"\\nRequired dependencies:\")\n",
    "        print(\"- pip install langchain langchain-community\")\n",
    "        print(\"- pip install chromadb\")\n",
    "        print(\"- pip install sentence-transformers\")\n",
    "        print(\"- pip install numpy scikit-learn\")\n",
    "        print(\"- pip install spacy\")\n",
    "        print(\"- python -m spacy download en_core_web_sm\")\n",
    "        print(\"- pip install transformers\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Summary and Key Techniques\n",
    "\n",
    "\n",
    "\n",
    " This Enhanced RAG System demonstrates several advanced techniques:\n",
    "\n",
    "\n",
    "\n",
    " 1. **Multi-Strategy Document Processing**:\n",
    "\n",
    "    - Semantic chunking using NLP\n",
    "\n",
    "    - Enhanced traditional chunking with better separators\n",
    "\n",
    "    - Dynamic keyword indexing for fast search\n",
    "\n",
    "\n",
    "\n",
    " 2. **Hybrid Search Approach**:\n",
    "\n",
    "    - Vector similarity search using embeddings\n",
    "\n",
    "    - Keyword-based search with context extraction\n",
    "\n",
    "    - Semantic search using sentence transformers\n",
    "\n",
    "\n",
    "\n",
    " 3. **Intelligent Answer Generation**:\n",
    "\n",
    "    - Two-stage answer refinement process\n",
    "\n",
    "    - Context-aware answer enhancement\n",
    "\n",
    "    - Multi-source information synthesis\n",
    "\n",
    "\n",
    "\n",
    " 4. **Robust Architecture**:\n",
    "\n",
    "    - Fallback mechanisms for optional dependencies\n",
    "\n",
    "    - Comprehensive error handling\n",
    "\n",
    "    - Modular design for easy extension\n",
    "\n",
    "\n",
    "\n",
    " The system provides accurate, context-aware answers by leveraging both traditional information retrieval techniques and modern NLP approaches.\n",
    "\n",
    "\n",
    "\n",
    " [Back to Contents](#table-of-contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
